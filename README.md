# Introduction
5.34 million standing homes sold in 2019, with an additional 682,000 new homes being sold. Over 6 million homes traded hands that year. That is over 16,000 homes sold every day. The project modelsand predicts future housing prices based on pricing data from Zillow, a real estate website. Then using the prediction, along with additional data, integrate an investment strategy that chooses which zip codes represent the best investment opportunity. This strategy will take a buyers approach and ask what interests a buyer in a home. Understanding buyer behavior will incorporate the human aspect into the strategy, allowing the scheme to be more accurate.  I used python for data wrangling, modeling and visualization.

# Data
Zillow is a leading real estate investment site that boasts of “incorporating data to empower the consumer.”  For many consumers, the availability of data on the Zillow site make buying and selling homes easier.  In order to leverage all the data available, Zillow created an internal organization called Zillow Research.  The goal of Zillow Research is to create free, unbiased data. 

One of the most important collections of data within Zillow Research (ZR) is the median house price.  ZR presents the median house sales price over time from 1996 -2020 by zip code. Each zip code represents an individual time series. Not all zip codes have data for every date, so those values are NULL. The Zillow data, 30,000 zip codes, will act as a baseline of zip codes for the other metrics.

In addition to the objective sales data, we will include some additional data that might better reflect buyer behavior.  For a broad approach to the project, the data collected will be not only objective but also subjective. Choosing a home is not solely based on the price of the house. People prefer to be in specific neighborhoods, with excellent schools and close amenities. One way to accomplish modeling buyer behavior and learning how to harness data is to scrape the internet for data to incorporate into the investment model. Median age, livability, comfort, walkability, population, and population change will all be additional metrics. These additional metrics give a good sense of buyer's behavior, which is a good indicator of where to invest. 

# Methodology
There are many solutions to creating an investment strategy. The most natural solution would to only use two metrics on which to base the decision. Each axis represents a variable forming a 2D square. Each quadrant of the graph would then be associated with a type of investment. One quadrant would be optimal. When a zip code presents with both positive variables, it would be an optimal investment. Visually this can be done with a maximum of three variables. After three metrics, the graph becomes hard to visualize. These methods restrict the amount of information from which the investment strategy draws. More detailed metrics, like those introduced in the data section, offers a more insightful investment strategy. The strategy uses a radar chart, more formally called a spider chart, to incorporate more metrics. While a radar chart will be a helpful visual tool for comparing zip codes, the most critical portion of the graph is the area of the graph. The area of the radar chart is the final determinator of which zip codes are suitable investments—the larger the area, the better investment.

# Data Collection
The Median house prices were given by zip code from Zillow. To obtain the rest of the metrics, the coded first gathered all the zip codes presented in the Zillow data. Selenium and BeautifulSoup, both python libraries, performed the necessary web scaping.  Selenium searched each zip code in the data set on the desired web page. After navigating to specific zip codes web pages, BeautifulSoup scraped individual pages for the desired data point.  Three thousand zip codes were run at a time, not to overload the computer. Each three thousand data points collected in a CSV file that was later combined to form a complete record for the metric. Using the web driver caused a few errors. The code missed some zip codes, even with wait statements, because of the speed of the code. Each missed zip code was added to an error list and ran again. The web pages did not contain a portion of the zip codes. If the zip code was not present, it was given a 0 as a score. It is reasonable to assume that the area is not significant enough or does not qualify for the website. 

# Modeling 

A well-tuned autoregression model would most likely produce a more specific and accurate result, it takes time and resources to tune the model. Prophet, and additive model, requires no tuning because it is robust to outliers and missing data. Many of the zip codes do not have data reported for many years. Prophet will be able to handle the missing data with ease. Prophet is also fast. It can model a single zip code in about 5-10 seconds, while a more sophisticated model would take much longer. With over 30,000 zip codes to model, time is a necessity. Prophet allows for speedier computation while retaining accuracy. Prophet performs best if there is strong seasonality. If the seasonality is constant, it is still strong. Home sales drop in the winter and pick back up in the summer. The modeling was done in 3000 zip code increments to ensure it went smoothly. Zillow provides a data set with zip codes as a column, 'RegionName,' and the time series as the rows.

Prophet requires two columns to run: "ds" and "y." The model collected MSE, MAE, and r2 from the forecast. The error I used in the radar chart will be the RMSE or the square root of the MSE. RMSE is a measure of how far the points of the times series are from the trend line. The farther away the points the higher the RMSE, signifying the trend missed the point. The population change is the predicted value minus the last known population. Percent change will keep larger cities from dominating within the strategy. Percent change is total population change divided by the last known population then multiplying it by 100. This metric will offer a view of the area's growth.

Radar ChartAll the zip code data files were combined into a single file, using the glob library and the panda's library. Each file was compared the zip codes from each file to the original Zillow dataset, to ensure that data contained each zip code. Once the total number of zip codes was 30,464 zip codes, the data set was ready. 

After scaling the data, it was time to create the radar chart for each zip code. First, code collected all the data into a single DataFrame. Then, the matplotlib library, available in python, is used to create the radar chart (Holtz, Lasseter). First, the code finds the number of variables so that there is the correct number of points in the graph. Based on the number of variables, the code calculates the correct angles, where the data points will sit. 
	
The code calculates the values for each zip code and then creates the radar chart. As mentioned early, the area of the radar chart is used as the determination of suitable investments, the higher the area, the better investment. Using a radar chart allows the strategy to be modified to user preference. If the user wants the scales to be different because they value some metrics over the other, they are easily interchanged. 

The area of the radar chart is a good indication of the scores of the metrics. As the metric grows so will the area of the chart. A zip code that scores well on all the metric will have a big area. Since the metrics are chosen to reflect a good investment, a large area will mean a good investment. 
To find the area, first, the x and y coordinates for each of the zip code values were created. The x and y coordinate can be created from angles and points values using trigonometry. 

The code obtains the coordinate set using cosine for the x coordinate and sin for the y coordinate. The shoelace method was then applied to the collection of x and y values to calculate the area. The shoelace theorem is a formula for finding the area of a polygon. The theorem is presented in the appendix. Here is the equation (“Page”).
The heat map was created using the folium, pandas, and json packages available in python (Python-Visualization). First all the zip codes were grouped by state code, i.e. ‘NY’. Then the average of all the areas was taken. This average was then applied to a json U.S. state file. The color gradient of blue was used to depict the states ranking.
