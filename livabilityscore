import numpy as np
import pandas as pd
import requests
import bs4
import lxml.etree as xml
from selenium import webdriver
import os
import time
from selenium.webdriver.common.keys import Keys
from google.colab import files

r = requests.get(URL)

soup = bs4.BeautifulSoup(r.content, "html.parser")

myclass = soup.find_all('i', {'class': 'scr'})

print(int(myclass[0].getText()))

#IMPORTING DATA
file = pd.read_csv("/content/zip_codes.csv")

#CREATING DATAFRAME
data = pd.DataFrame(file)
print(data.head())

#removing useless columns
col = ["StateName", "State", "City", "Metro", "CountyName", "SizeRank", 
       "RegionType", "RegionID"]
data = data.drop(columns = col )
print(data.head())

zips = data['RegionName'].tolist()

zips = zips[12001:15000]

options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome('chromedriver',options=options)

live_scores = {}
err_list = []

for query in zipcodes:
  try:
    thing_url = "https://www.areavibes.com/"
    driver.get(thing_url)
    time.sleep(1)

    searchbox = driver.find_element_by_xpath("/html/body/div[2]/div/div/div/div/div/form[2]/input")
    searchbox.click()
    searchbox.send_keys(str(query))
    time.sleep(1)
    searchbox.send_keys(Keys.ENTER)

    window_after = driver.window_handles[0]
    driver.switch_to.window(window_after)
    time.sleep(1)

    page = driver.current_url
    r = requests.get(page)
    soup = bs4.BeautifulSoup(r.content, "html.parser")

    myclass = soup.find_all('i', {'class': 'scr'})

    live_scores[query] = int(myclass[0].getText())

   
  except Exception as e:
    print(e)
    live_scores[query] = 0


df = pd.DataFrame.from_dict(data=live_scores, orient='index')
df.to_csv('livescores18.csv')

er = pd.DataFrame(err_list)
er.to_csv('liveerr18.csv')

files.download("livescores18.csv")
files.download('liveerr18.csv')
